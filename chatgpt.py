# -*- coding: utf-8 -*-
"""chatgpt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1doufsOHMhQtfuU947M6l82oviY_YzwTp
"""

pip install transformers

pip install tensorflow

from tensorflow.keras.callbacks import ReduceLROnPlateau



import tensorflow as tf

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import BertTokenizer, TFBertModel
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ReduceLROnPlateau

# Load the train and test datasets
train = pd.read_csv("Train (9).csv")
test = pd.read_csv("Test (9).csv")

pip install noel

import nltk
nltk.download('stopwords')

# Prepare the data
corpus = []
swahili_stopwords = set([
    "akasema", "alikuwa", "alisema", "baada", "basi", "bila", "cha", "chini", "hadi", "hapo",
    "hata", "hivyo", "hiyo", "huko", "huo", "ili", "ilikuwa", "juu", "kama", "karibu", "katika",
    "kila", "kima", "kisha", "kubwa", "kutoka", "kuwa", "kwa", "kwamba", "kwenda", "kwenye", "la",
    "lakini", "mara", "mdogo", "mimi", "mkubwa", "mmoja", "moja", "muda", "mwenye", "na", "naye",
    "ndani", "ng", "ni", "nini", "nonkungu", "pamoja", "pia", "sana", "sasa", "sauti", "tafadhali",
    "tena", "tu", "vile", "wa", "wakati", "wake", "walikuwa", "wao", "watu", "wengine", "wote",
    "ya", "yake", "yangu", "yao", "yeye", "yule", "za", "zaidi", "zake"
])

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

for i in range(len(train)):
    review = re.sub('[^a-zA-Z]', ' ', train['content'][i])
    review = review.lower()
    review = review.split()
    review = [word for word in review if not word in swahili_stopwords]
    review = ' '.join(review)
    corpus.append(review)

# Define the maximum sequence length you want to use
max_sequence_length = 64

# Encode the sentences using BERT tokenizer with the specified max_length
encoded_inputs = tokenizer(corpus, padding=True, truncation=True, max_length=max_sequence_length, return_tensors='tf')


# Prepare the target variable
encoder = LabelEncoder()
y = encoder.fit_transform(train['category'])
y = to_categorical(y)

# Split the data into train and test splits
X_train, X_test, y_train, y_test = train_test_split(np.array(encoded_inputs['input_ids']),np.array(y),test_size=0.33,random_state=42)

# Load the pre-trained BERT model
bert_model = TFBertModel.from_pretrained('bert-base-uncased')

# Fine-tune the entire BERT model
bert_model.trainable = True



# Define the model architecture
inputs = Input(shape=(None,), dtype=tf.int32)
bert_output = bert_model(inputs)[0]
output = Dense(5, activation='softmax')(bert_output[:, 0, :])  # Use only the first token's representation
model = Model(inputs=inputs, outputs=output)

# Set up the learning rate scheduler
reduce_lr_callback = ReduceLROnPlateau(
    monitor='val_loss',  # Monitor validation loss
    factor=0.1,           # Reduce learning rate by a factor of 0.1 when the monitored metric plateaus
    patience=2,           # Number of epochs with no improvement after which learning rate will be reduced
    min_lr=1e-8           # Minimum learning rate (to prevent it from getting too small)
)

# Compile the model with optimizer and loss function
optimizer = Adam(learning_rate=1e-5)
loss_fn = tf.keras.losses.CategoricalCrossentropy()
model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])

model.summary()

# Print the gradients of the model's trainable variables
inputs = tf.convert_to_tensor(X_train[:1])
targets = tf.convert_to_tensor(y_train[:1])
with tf.GradientTape() as tape:
    predictions = model(inputs)
    loss = loss_fn(targets, predictions)

gradients = tape.gradient(loss, model.trainable_variables)

# Train the model with the learning rate scheduler callback
model.fit(
    X_train,
    y_train,
    validation_data=(X_test, y_test),
    epochs=10,
    batch_size=32,
    callbacks=[reduce_lr_callback]  # Pass the learning rate scheduler callback
)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=32)
print("Test Loss:", test_loss)
print("Test Accuracy:", test_accuracy)

# Generate predictions on the test set
test_encoded_inputs = tokenizer(test['content'].tolist(), padding=True, truncation=True, max_length=max_sequence_length, return_tensors='tf')
test_predictions = model.predict(test_encoded_inputs['input_ids'])



# Convert predictions to the submission format
submission = pd.DataFrame(data=test_predictions, columns=encoder.classes_)
submission.insert(0, 'test_id', test['swahili_id'])  # Use the correct column name for test IDs
submission.to_csv('gpt.csv', index=False)

from google.colab import files
files.download('gpt.csv')